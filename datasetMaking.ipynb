{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 84 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/lib/python3/dist-packages (from gensim) (1.14.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 893 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.18.1\n",
      "  Downloading scipy-1.5.3-cp38-cp38-manylinux1_x86_64.whl (25.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.8 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107092 sha256=4d478dc52e0c1c0a8be3538f3d46eb12dc1832925504a6620ced768b98fdf4e3\n",
      "  Stored in directory: /root/.cache/pip/wheels/11/73/9a/f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "Successfully installed gensim-3.8.3 scipy-1.5.3 smart-open-3.0.0\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.3 MB 104 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3/dist-packages (from pandas) (2019.3)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.4\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 667 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Collecting joblib\n",
      "  Downloading joblib-0.17.0-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2020.10.28-cp38-cp38-manylinux2010_x86_64.whl (679 kB)\n",
      "\u001b[K     |████████████████████████████████| 679 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 647 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=068a2e2395ffdbd5280ef69c22876d0b9304b26b561b2f0daa47ccf3dd22d176\n",
      "  Stored in directory: /root/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "Successfully built nltk\n",
      "Installing collected packages: joblib, regex, tqdm, nltk\n",
      "Successfully installed joblib-0.17.0 nltk-3.5 regex-2020.10.28 tqdm-4.51.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim\n",
    "!pip3 install pandas\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ready!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"mention_overall_dict.pickle\", \"rb\") as f:\n",
    "    mention_to_entities = pickle.load(f)\n",
    "print (\"all ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'class', 'is', 'a', 'blueprint', 'for', 'the', 'object.', 'A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object.', 'A class is', 'class is a', 'is a blueprint', 'a blueprint for', 'blueprint for the', 'for the object.', 'A class is a', 'class is a blueprint', 'is a blueprint for', 'a blueprint for the', 'blueprint for the object.', 'A class is a blueprint', 'class is a blueprint for', 'is a blueprint for the', 'a blueprint for the object.', 'A class is a blueprint for', 'class is a blueprint for the', 'is a blueprint for the object.', 'A class is a blueprint for the', 'class is a blueprint for the object.', 'A class is a blueprint for the object.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "s_words=[ ' '.join(grams) for grams in list(everygrams('A class is a blueprint for the object.'.split()))]\n",
    "print( s_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Toni%20Morrison': 91})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_to_entities[\"Toni Morrison\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'John%20Robert%20Morrison': 1,\n",
       "         'Herbert%20Morrison': 1,\n",
       "         'Morrison%2C%20Virginia': 1,\n",
       "         'Morrison%2C%20Illinois': 1,\n",
       "         'Morrison%2C%20Colorado': 9,\n",
       "         'Morrison%20County%2C%20Minnesota': 3,\n",
       "         'Morrison%20%28community%29%2C%20Wisconsin': 1,\n",
       "         'Morrison%2C%20Missouri': 1,\n",
       "         '%23Morrison%20shelter': 1,\n",
       "         'Bruce%20Morrison': 1,\n",
       "         'Morrison%20Bridge': 2,\n",
       "         'Morrison%20Formation': 1,\n",
       "         'Morrison%2C%20OK': 1,\n",
       "         'Morrison%20%28surname%29': 1,\n",
       "         'Morrison%20Government': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_to_entities[\"Morrison\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Toni%20%281935%20film%29': 2,\n",
       "         'Antonio%20Mu%C3%B1oz%20G%C3%B3mez': 2,\n",
       "         'Toni%20Adams': 1,\n",
       "         'Toni%20%28footballer%2C%20born%201946%29': 2,\n",
       "         'Toni%20Tennille': 1,\n",
       "         'Toni%20Braxton': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mention_to_entities[\"Toni\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "from nltk.util import everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since beginning: 00:46:32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file_object = open('results/segment0.txt','a+', encoding='utf-8')\n",
    "\n",
    "start_time = time.time()\n",
    "lineCount=0\n",
    "\n",
    " # iterate over the plain text data we just created\n",
    "with utils.open('enwiki-latest.json.gz', 'rb') as f:\n",
    "     for line in range(70000):\n",
    "#      for line in f:\n",
    "            lineCount+=1;\n",
    "            # segmenting results to batches with size of 100000\n",
    "            # to avoid lost of  all articles for an unexcpected error\n",
    "            if(lineCount%100000==0):\n",
    "                file_object.close()\n",
    "                name=\"segment\"+str(lineCount/100000)+\"\"\n",
    "                file_object = open('results/'+name+'.txt','a+', encoding='utf-8')\n",
    "                \n",
    "            # decode each JSON line into a Python dictionary object (each line is one article)\n",
    "            article = json.loads(next(f))\n",
    "            # each article has:\n",
    "            # title\n",
    "            # inter links\n",
    "            # sections \n",
    "            title=article['title']\n",
    "            p_links= article['interlinks']\n",
    "            all_links_till_here=set()\n",
    "            # link[1] refers to metion of entitiy in text\n",
    "            # link[0] refers to the entity\n",
    "            for link in p_links:\n",
    "                link.append([ ' '.join(grams) for grams in list(everygrams(link[1].split()))])\n",
    "            \n",
    "            for section_text in article['section_texts']:\n",
    "                sentences = sent_tokenize(section_text)\n",
    "                for sentence in sentences:\n",
    "                    entities=[]\n",
    "                    #first try to find entities which are mentioned  by wiki in sentence\n",
    "                    for link in p_links:\n",
    "                        if(link[1] in sentence):\n",
    "                            all_links_till_here.add(link[0])\n",
    "                            entities.append(link[0].replace(' ','_'))\n",
    "                        #try to find entities which arent mentioned  by wiki for avoidance of reputation\n",
    "                        # mention_to_entities contains collection of all entities that are exists for a mention\n",
    "                        for ngram_link in link[2]:\n",
    "                            if(ngram_link in sentence):\n",
    "                                try:\n",
    "                                    if((quote(link[0])in mention_to_entities[ngram_link]) and (link[0] in all_links_till_here)):\n",
    "                                        entities.append(link[0].replace(' ','_'))\n",
    "                                        all_links_till_here.add(link[0])\n",
    "                                except:\n",
    "                                    pass\n",
    "\n",
    "                    if(len(entities)>0):\n",
    "                        # writing each triple in a line (from_entity,sentence,to_entities)\n",
    "                        file_object.write(\"from_entity:\"+title+\"\")\n",
    "                        file_object.write('\\n')\n",
    "                        file_object.write(\" sentence:\"+sentence)\n",
    "                        file_object.write('\\n') \n",
    "                        file_object.write(\"to_entities:\"+\",\".join(list(set(entities))))\n",
    "                        file_object.write('\\n')\n",
    "                        \n",
    "             \n",
    " \n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "since_beginning = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "print(\"Since beginning: %s\" % (since_beginning))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
